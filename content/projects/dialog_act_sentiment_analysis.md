---
title: "Fine-tuning Pre-trained Language Models for Dialog Act Classification and Sentiment Analysis" 
date: 2023-03-21
order: 2
show_date: false
url: /dialog_act_sentiment_analysis/
aliases: 
    - /dialog_act_sentiment_analysis.html
tags: ["Natural Language Processing", "Classification", "Dialog Act", "Sentiment Analysis"]
author: "Mélissa Tamine, in a group work"
summary: "This work presents an approach for fine-tuning pre-trained language models to perform dialog act classification or sentiment/emotion analysis."
cover:
    image: "/projects/da_sa_classification.png#center"
    alt: ""
    relative: true 

---

---

##### Link

+ [GitHub](https://github.com/taminemelissa/intent-classification)

---

##### Abstract

This work presents an approach for fine-tuning pre-trained language models to perform Dialog Act Classification or Sentiment/Emotion analysis. We start with a pre-trained language model and fine-tune it on task-specific datasets from SILICONE [1] using transfer learning techniques. Our approach improves the model’s ability to capture task-specific nuances while retaining its pre-existing language understanding capabilities. We experiment with different pre-trained models and compare their performances. We also perform undersampling on training data to evaluate the performance gain associated with data balancing. Overall, our findings suggest that fine-tuning pre-trained language models is an effective approach for improving the performance of Dialog Act classification and Sentiment Analysis models.

---

##### Collaborators

This work has been done in a group work with [Théo Lorthios](https://www.linkedin.com/in/th%C3%A9o-lorthios-963b3b166/), under the supervision of [Pierre Colombo](https://pierrecolombo.github.io/) as part of the _Natural Language Processing_ course at ENSAE Paris.

---

##### References

[1] Chapuis, E., Colombo, P., Manica, M., Labeau, M., & Clavel, C. (2021). Hierarchical Pre-training for Sequence Labelling in Spoken Dialog.
