---
title: "Quiz Generator" 
date: 2021-12-22
order: 4
show_date: false
url: /quiz_generator/
aliases: 
    - /quiz_generator.html
tags: ["Natural Language Processing", "Generative Models", "Question Answering Task"]
author: "Mélissa Tamine, in a group work"
summary: "Recent research work has highlighted the growing effectiveness of generative language models in the production of accurate and coherent textual content. By exploiting this technology, this project aimed to develop a system capable of automatically composing questions and answers on predefined topics." 
cover:
    image: ""
    alt: ""
    relative: true

---

---

##### Links

+ [GitHub](https://github.com/taminemelissa/multi-label-classification)
+ [Notebook](https://github.com/taminemelissa/quiz-generator/blob/main/final-project.ipynb) [French]

---

##### Abstract

This study addressed the challenge of devising a language model pipeline proficient in generating tailored quizzes comprising multiple question-answer pairs on specific themes. Recent advancements in natural language processing showcase the capabilities of language models in generating coherent and contextually relevant text [1][2][3]. Leveraging this progress, the focus was on constructing a dynamic pipeline integrating multiple language models to facilitate the generation of diverse and accurate quiz content. The primary objective was to design a system that amalgamates the strengths of various language models, optimizing their collective abilities to generate comprehensive and educative quiz sets. This project was carried out as part of the _Python programming_ course at ENSAE Paris. It addresses data processing, data visualization and modeling works. 

---

##### Collaborators

This work has been done in a group work with [Adrien Servière](https://www.linkedin.com/in/adrien-serviere/), under the supervision of [Kim Antunez](https://www.linkedin.com/in/kantunez/?originalSubdomain=fr) from [INSEE](https://www.insee.fr/fr/accueil) as part of the _Python Programming_ course at ENSAE Paris.

---

##### References

[1] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners.

[2] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2019). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.

[3] Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C., & Socher, R. (2019). CTRL: A Conditional Transformer Language Model for Controllable Generation.